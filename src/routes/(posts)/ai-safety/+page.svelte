<svelte:head>
	<title>My take on AI safety</title>
</svelte:head>
<h1>My take on AI safety</h1>
<div class="opacity-60 mb-8">February 2023</div>
<p>
	This is my uninformed perspective as an engineer in the LLM space. Others have thought much more
	deeply about this than me, so you shouldn’t listen to me.
</p>
<p>
	The mainstream seems to believe the primary risks of AI are fake content and unemployment. I think
	these are real problems, but relatively easy to manage. Cryptographic digital signatures should
	provide an antibody to fake content, and wealth redistribution methods should help those who’ve
	lost their economic value. I suspect in the future, people will be wondering why they cared so
	much about making money rather than simply doing what they enjoy.
</p>
<p>
	Researchers are primarily concerned with existential risk. The way I’ve mostly seen it presented
	is that it’s impossible to align a superintelligent AI, so it will do whatever it wants, which
	more likely than not will end up killing us. While this seems plausible to me, I haven’t found an
	argument for it that I understood. It seems to me that if you took a good, empathetic human and
	made them a billion times more intelligent, they would still avoid hurting humans. After all,
	we’re able to feel abundant empathy for the mentally disabled. It’s unclear to me why a
	superintelligent AI trained by humans will have different values than a superintelligent human. Is
	the idea that the superintelligent human would find easier ways to satisfy their reward systems,
	like by sticking electrodes into their brains?
</p>
<p>
	Regardless, I think existential risk will come long before AI is smart enough to unalign itself
	with its developers, because some developers will be unaligned. We’ve already had close calls with
	nuclear weapons and synthetic viruses. I think the reason those weapons haven’t ended the world
	yet is because they’re hard to use tactically—it’s hard to get them to accomplish what a group of
	powerful people actually want. Not many people want a city reduced to rubble, and we can’t control
	what a virus is going to mutate into as it spreads. AI is considerably more useful. You can use it
	to manipulate and subjugate precisely the subset of society you want to target.
</p>
<p>
	There will almost certainly be more good people using AI than bad people using AI, so they’ll be
	trying to stop the abusers. But this doesn’t bring me much comfort because it’s far easier to
	create destruction than it is to stop it—again, consider nuclear weapons and synthetic viruses.
</p>
<p>
	I have no solutions! Would love to discuss with anyone who can think of solutions or poke holes in
	my logic.
</p>
